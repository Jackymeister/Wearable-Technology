{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import operator\n",
    "from pprint import pprint\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jackymeister/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jackymeister/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "#print(stop_words)\n",
    "nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "#print(nltk_words)\n",
    "stop_words.extend(nltk_words)\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>RelatedActiveTopics</th>\n",
       "      <th>PlatformType</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The business, function and art of textiles | T...</td>\n",
       "      <td>2020-01-15 23:32:18</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Milly &amp;amp; Roots, The Headscarf. A children's...</td>\n",
       "      <td>2020-01-10 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Pamex 2020: Epson - A market leader in textile...</td>\n",
       "      <td>2020-01-07 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Pamplin Media Group - K's Clothing Boutique a ...</td>\n",
       "      <td>2020-01-13 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>From selling shirts to owning ‘Tuned in Tokyo’...</td>\n",
       "      <td>2020-01-02 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text  \\\n",
       "0           1  The business, function and art of textiles | T...   \n",
       "1           2  Milly &amp; Roots, The Headscarf. A children's...   \n",
       "2           3  Pamex 2020: Epson - A market leader in textile...   \n",
       "3           4  Pamplin Media Group - K's Clothing Boutique a ...   \n",
       "4           5  From selling shirts to owning ‘Tuned in Tokyo’...   \n",
       "\n",
       "             Timestamp RelatedActiveTopics    PlatformType    Platform  Year  \n",
       "0  2020-01-15 23:32:18       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "1  2020-01-10 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "2  2020-01-07 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "3  2020-01-13 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "4  2020-01-02 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Twitter = pd.read_csv(\"Google.csv\")\n",
    "Twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>RelatedActiveTopics</th>\n",
       "      <th>PlatformType</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The business, function and art of textiles | T...</td>\n",
       "      <td>2020-01-15 23:32:18</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Milly &amp;amp; Roots, The Headscarf. A children's...</td>\n",
       "      <td>2020-01-10 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Pamex 2020: Epson - A market leader in textile...</td>\n",
       "      <td>2020-01-07 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Pamplin Media Group - K's Clothing Boutique a ...</td>\n",
       "      <td>2020-01-13 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>From selling shirts to owning ‘Tuned in Tokyo’...</td>\n",
       "      <td>2020-01-02 18:00:00</td>\n",
       "      <td>digital cloth</td>\n",
       "      <td>NewsAggregator</td>\n",
       "      <td>GoogleNews</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text  \\\n",
       "0           1  The business, function and art of textiles | T...   \n",
       "1           2  Milly &amp; Roots, The Headscarf. A children's...   \n",
       "2           3  Pamex 2020: Epson - A market leader in textile...   \n",
       "3           4  Pamplin Media Group - K's Clothing Boutique a ...   \n",
       "4           5  From selling shirts to owning ‘Tuned in Tokyo’...   \n",
       "\n",
       "             Timestamp RelatedActiveTopics    PlatformType    Platform  Year  \n",
       "0  2020-01-15 23:32:18       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "1  2020-01-10 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "2  2020-01-07 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "3  2020-01-13 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  \n",
       "4  2020-01-02 18:00:00       digital cloth  NewsAggregator  GoogleNews  2020  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Twitter = Twitter.sample(n = 3000)\n",
    "Twitter = Twitter.reset_index(drop = True)\n",
    "Twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2624"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Twitter = Twitter.drop_duplicates(\"Text\")\n",
    "len(Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_Text = Twitter[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "T_Text[\"Index\"] = Twitter.set_index\n",
    "T_Text = pd.DataFrame(T_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The business, function and art of textiles | T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Milly &amp;amp; Roots, The Headscarf. A children's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Pamex 2020: Epson - A market leader in textile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Pamplin Media Group - K's Clothing Boutique a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From selling shirts to owning ‘Tuned in Tokyo’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  The business, function and art of textiles | T...\n",
       "1  Milly &amp; Roots, The Headscarf. A children's...\n",
       "2  Pamex 2020: Epson - A market leader in textile...\n",
       "3  Pamplin Media Group - K's Clothing Boutique a ...\n",
       "4  From selling shirts to owning ‘Tuned in Tokyo’..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_Text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<ipython-input-16-e26ed5b07c42>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "  T_Text.loc[:,\"Text\"] = T_Text.Text.apply(lambda x : \" \".join(re.findall('\\w+',str(x)))) # remove punctuation\n",
      "<ipython-input-16-e26ed5b07c42>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "  T_Text[\"Text\"] = T_Text.Text.str.replace('\\d+', '') # remove numbers\n"
     ]
    }
   ],
   "source": [
    "T_Text = T_Text[T_Text['Text'].notnull()]\n",
    "T_Text.loc[:,\"Text\"] = T_Text.Text.apply(lambda x : \" \".join(re.findall('\\w+',str(x)))) # remove punctuation\n",
    "T_Text.loc[:,\"Text\"] = T_Text.Text.apply(lambda x : str.lower(x)) # lower case\n",
    "T_Text[\"Text\"] = T_Text.Text.str.replace('\\d+', '') # remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopWords(s):\n",
    "    s = ' '.join(word for word in s.split() if word not in stop_words)\n",
    "    return s\n",
    "T_Text.loc[:,\"Text\"] = T_Text.Text.apply(lambda x: remove_stopWords(x)) # stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "T_Text.loc[:,\"Text\"] = T_Text.Text.apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "T_Text.loc[:,\"Text\"] = T_Text.Text.apply(lemm_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[busi, function, art, textil, time, georgian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[milli, amp, root, headscarf, child, book, dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[pamex, epson, market, leader, textil, print, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[pamplin, medium, group, k, cloth, boutiqu, gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[sell, shirt, own, tune, tokyo, franki, quiroz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  [busi, function, art, textil, time, georgian, ...\n",
       "1  [milli, amp, root, headscarf, child, book, dea...\n",
       "2  [pamex, epson, market, leader, textil, print, ...\n",
       "3  [pamplin, medium, group, k, cloth, boutiqu, gr...\n",
       "4  [sell, shirt, own, tune, tokyo, franki, quiroz..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_Text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(T_Text[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 art\n",
      "1 busi\n",
      "2 function\n",
      "3 georgian\n",
      "4 textil\n",
      "5 time\n",
      "6 amp\n",
      "7 book\n",
      "8 child\n",
      "9 deal\n",
      "10 digit\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.items():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2bow\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in T_Text[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using BOW Base Model\n",
    "lda_model_base = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "coherence_model_lda_base = CoherenceModel(model=lda_model_base, texts = T_Text[\"Text\"], dictionary = dictionary, coherence='c_v')\n",
    "coherence_lda_base = coherence_model_lda_base.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=13, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.9,\n",
    "                                           eta=0.1)\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = T_Text[\"Text\"], dictionary = dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=T_Text[\"Text\"], dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [2:05:32, 22.82s/it]                     \n"
     ]
    }
   ],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 5\n",
    "max_topics = 16\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(bow_corpus)\n",
    "corpus_sets = [#gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               #gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               #gensim.utils.ClippedCorpus(bow_corpus, num_of_docs*0.75), \n",
    "               bow_corpus]\n",
    "corpus_title = ['100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total = 50)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=bow_corpus[i], dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results_Google.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, higher alpha values mean documents contain more similar topic contents. The same is true for beta, but with topics and words: generally a high beta will result in topics with more similar word contents. Also, an asymmetric alpha is helpful, where as an asymmetric beta is largely not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we decide num_topics by controling alpha and beta.\n",
    "Then we get our number of topics, then we decide alpha and beta to get our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.021*\"realiti\" + 0.011*\"virtual\" + 0.010*\"augment\" + 0.008*\"ar\" + 0.007*\"vr\" + 0.007*\"digit\" + 0.006*\"said\" + 0.005*\"game\" + 0.004*\"world\" + 0.004*\"experi\" + 0.004*\"work\" + 0.004*\"one\" + 0.004*\"develop\" + 0.003*\"mix\" + 0.003*\"glass\" + 0.003*\"busi\" + 0.003*\"real\" + 0.003*\"see\" + 0.003*\"eye\" + 0.003*\"prosthesi\" + 0.003*\"headset\" + 0.003*\"design\" + 0.003*\"say\" + 0.003*\"peopl\" + 0.003*\"need\" + 0.003*\"creat\" + 0.003*\"first\" + 0.002*\"way\" + 0.002*\"ago\" + 0.002*\"prosthet\" + 0.002*\"user\" + 0.002*\"vision\" + 0.002*\"content\" + 0.002*\"look\" + 0.002*\"industri\" + 0.002*\"news\" + 0.002*\"team\" + 0.002*\"custom\" + 0.002*\"go\" + 0.002*\"servic\" + 0.002*\"tech\" + 0.002*\"train\" + 0.002*\"face\" + 0.002*\"right\" + 0.002*\"app\" + 0.002*\"brand\" + 0.002*\"healthcar\" + 0.002*\"hour\" + 0.002*\"offer\" + 0.002*\"take\"\n",
      "Topic: 1 \n",
      "Words: 0.007*\"one\" + 0.005*\"said\" + 0.004*\"smart\" + 0.004*\"look\" + 0.004*\"say\" + 0.004*\"go\" + 0.003*\"show\" + 0.003*\"want\" + 0.003*\"peopl\" + 0.003*\"come\" + 0.003*\"first\" + 0.003*\"clean\" + 0.003*\"work\" + 0.003*\"game\" + 0.003*\"phone\" + 0.003*\"cloth\" + 0.003*\"even\" + 0.003*\"way\" + 0.003*\"right\" + 0.003*\"day\" + 0.003*\"home\" + 0.003*\"two\" + 0.002*\"take\" + 0.002*\"made\" + 0.002*\"thing\" + 0.002*\"best\" + 0.002*\"design\" + 0.002*\"back\" + 0.002*\"around\" + 0.002*\"news\" + 0.002*\"last\" + 0.002*\"see\" + 0.002*\"think\" + 0.002*\"woman\" + 0.002*\"may\" + 0.002*\"love\" + 0.002*\"need\" + 0.002*\"person\" + 0.002*\"hand\" + 0.002*\"good\" + 0.002*\"call\" + 0.002*\"keep\" + 0.002*\"digit\" + 0.002*\"edit\" + 0.002*\"world\" + 0.002*\"much\" + 0.002*\"mani\" + 0.002*\"know\" + 0.002*\"live\" + 0.002*\"fashion\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"fit\" + 0.011*\"band\" + 0.010*\"amazon\" + 0.010*\"watch\" + 0.009*\"smart\" + 0.008*\"deal\" + 0.007*\"tracker\" + 0.007*\"price\" + 0.006*\"featur\" + 0.006*\"smartwatch\" + 0.006*\"realm\" + 0.006*\"track\" + 0.005*\"one\" + 0.005*\"appl\" + 0.005*\"devic\" + 0.005*\"view\" + 0.005*\"best\" + 0.004*\"come\" + 0.004*\"app\" + 0.004*\"fitbit\" + 0.004*\"offer\" + 0.004*\"batteri\" + 0.004*\"day\" + 0.004*\"launch\" + 0.004*\"rate\" + 0.004*\"look\" + 0.004*\"display\" + 0.004*\"charg\" + 0.004*\"run\" + 0.004*\"sale\" + 0.004*\"mi\" + 0.003*\"heart\" + 0.003*\"well\" + 0.003*\"tv\" + 0.003*\"avail\" + 0.003*\"monitor\" + 0.003*\"inch\" + 0.003*\"screen\" + 0.003*\"samsung\" + 0.003*\"save\" + 0.003*\"pro\" + 0.003*\"right\" + 0.003*\"review\" + 0.003*\"activ\" + 0.003*\"insulin\" + 0.003*\"googl\" + 0.003*\"phone\" + 0.003*\"sleep\" + 0.003*\"design\" + 0.003*\"smartphon\"\n",
      "Topic: 3 \n",
      "Words: 0.034*\"ring\" + 0.014*\"data\" + 0.011*\"secur\" + 0.010*\"video\" + 0.010*\"user\" + 0.009*\"camera\" + 0.009*\"privaci\" + 0.008*\"devic\" + 0.007*\"inform\" + 0.007*\"doorbel\" + 0.007*\"home\" + 0.007*\"access\" + 0.006*\"app\" + 0.006*\"amazon\" + 0.006*\"smart\" + 0.006*\"said\" + 0.006*\"custom\" + 0.005*\"websit\" + 0.005*\"control\" + 0.005*\"person\" + 0.005*\"cooki\" + 0.004*\"servic\" + 0.004*\"polici\" + 0.004*\"may\" + 0.004*\"site\" + 0.004*\"polic\" + 0.004*\"wing\" + 0.003*\"third\" + 0.003*\"two\" + 0.003*\"system\" + 0.003*\"ad\" + 0.003*\"account\" + 0.003*\"parti\" + 0.003*\"track\" + 0.003*\"googl\" + 0.003*\"collect\" + 0.003*\"one\" + 0.003*\"share\" + 0.003*\"finger\" + 0.003*\"protect\" + 0.003*\"law\" + 0.003*\"advertis\" + 0.003*\"store\" + 0.003*\"surveil\" + 0.003*\"digit\" + 0.003*\"allow\" + 0.003*\"connect\" + 0.003*\"featur\" + 0.002*\"email\" + 0.002*\"address\"\n",
      "Topic: 4 \n",
      "Words: 0.010*\"robot\" + 0.006*\"research\" + 0.006*\"system\" + 0.005*\"digit\" + 0.005*\"develop\" + 0.005*\"design\" + 0.004*\"said\" + 0.004*\"smart\" + 0.004*\"human\" + 0.004*\"univers\" + 0.004*\"work\" + 0.004*\"water\" + 0.004*\"print\" + 0.004*\"materi\" + 0.003*\"industri\" + 0.003*\"one\" + 0.003*\"sensor\" + 0.003*\"world\" + 0.003*\"process\" + 0.003*\"team\" + 0.003*\"base\" + 0.003*\"project\" + 0.003*\"first\" + 0.003*\"control\" + 0.003*\"innov\" + 0.003*\"machin\" + 0.003*\"servic\" + 0.003*\"bodi\" + 0.003*\"bank\" + 0.003*\"news\" + 0.003*\"need\" + 0.003*\"oper\" + 0.003*\"energi\" + 0.003*\"made\" + 0.003*\"creat\" + 0.002*\"engin\" + 0.002*\"busi\" + 0.002*\"pump\" + 0.002*\"allow\" + 0.002*\"part\" + 0.002*\"skin\" + 0.002*\"studi\" + 0.002*\"exoskeleton\" + 0.002*\"support\" + 0.002*\"inform\" + 0.002*\"power\" + 0.002*\"build\" + 0.002*\"solut\" + 0.002*\"manag\" + 0.002*\"devic\"\n",
      "Topic: 5 \n",
      "Words: 0.025*\"devic\" + 0.021*\"wearabl\" + 0.010*\"medic\" + 0.010*\"smart\" + 0.007*\"health\" + 0.006*\"monitor\" + 0.006*\"consum\" + 0.006*\"data\" + 0.005*\"connect\" + 0.005*\"fit\" + 0.005*\"watch\" + 0.005*\"said\" + 0.005*\"glass\" + 0.005*\"appl\" + 0.005*\"digit\" + 0.005*\"increas\" + 0.004*\"ce\" + 0.004*\"user\" + 0.004*\"base\" + 0.004*\"peopl\" + 0.004*\"million\" + 0.004*\"ai\" + 0.004*\"offer\" + 0.003*\"heart\" + 0.003*\"system\" + 0.003*\"sensor\" + 0.003*\"healthcar\" + 0.003*\"electron\" + 0.003*\"tech\" + 0.003*\"improv\" + 0.003*\"activ\" + 0.003*\"develop\" + 0.003*\"share\" + 0.003*\"g\" + 0.003*\"smartwatch\" + 0.003*\"design\" + 0.003*\"alert\" + 0.003*\"well\" + 0.003*\"patient\" + 0.003*\"display\" + 0.003*\"com\" + 0.003*\"home\" + 0.003*\"servic\" + 0.003*\"vehicl\" + 0.003*\"come\" + 0.003*\"drive\" + 0.003*\"busi\" + 0.002*\"industri\" + 0.002*\"samsung\" + 0.002*\"rate\"\n",
      "Topic: 6 \n",
      "Words: 0.022*\"global\" + 0.017*\"analysi\" + 0.016*\"research\" + 0.015*\"industri\" + 0.012*\"region\" + 0.012*\"growth\" + 0.011*\"segment\" + 0.010*\"key\" + 0.010*\"forecast\" + 0.009*\"player\" + 0.009*\"applic\" + 0.009*\"com\" + 0.008*\"trend\" + 0.008*\"studi\" + 0.008*\"http\" + 0.008*\"smart\" + 0.007*\"type\" + 0.007*\"www\" + 0.006*\"data\" + 0.006*\"busi\" + 0.006*\"glass\" + 0.006*\"revenu\" + 0.006*\"share\" + 0.006*\"size\" + 0.005*\"competit\" + 0.005*\"manufactur\" + 0.005*\"sale\" + 0.005*\"develop\" + 0.005*\"sampl\" + 0.005*\"opportun\" + 0.005*\"america\" + 0.005*\"inform\" + 0.004*\"detail\" + 0.004*\"email\" + 0.004*\"chapter\" + 0.004*\"cover\" + 0.004*\"offer\" + 0.004*\"major\" + 0.004*\"factor\" + 0.004*\"insight\" + 0.004*\"custom\" + 0.004*\"overview\" + 0.004*\"valu\" + 0.004*\"asia\" + 0.004*\"countri\" + 0.003*\"system\" + 0.003*\"futur\" + 0.003*\"europ\" + 0.003*\"request\" + 0.003*\"strategi\"\n",
      "Topic: 7 \n",
      "Words: 0.070*\"smart\" + 0.035*\"wearabl\" + 0.022*\"tattoo\" + 0.022*\"shoe\" + 0.020*\"necklac\" + 0.018*\"devic\" + 0.013*\"sensor\" + 0.010*\"global\" + 0.009*\"news\" + 0.007*\"vuzix\" + 0.007*\"sport\" + 0.007*\"com\" + 0.007*\"releas\" + 0.007*\"band\" + 0.006*\"fit\" + 0.006*\"inc\" + 0.005*\"industri\" + 0.005*\"footwear\" + 0.005*\"research\" + 0.005*\"electron\" + 0.005*\"analysi\" + 0.004*\"swim\" + 0.004*\"corpor\" + 0.004*\"inform\" + 0.004*\"player\" + 0.004*\"nike\" + 0.004*\"share\" + 0.004*\"key\" + 0.004*\"growth\" + 0.004*\"www\" + 0.004*\"develop\" + 0.004*\"contact\" + 0.004*\"wireless\" + 0.004*\"busi\" + 0.003*\"data\" + 0.003*\"forecast\" + 0.003*\"eeg\" + 0.003*\"applic\" + 0.003*\"adida\" + 0.003*\"glass\" + 0.003*\"googl\" + 0.003*\"email\" + 0.003*\"consum\" + 0.003*\"america\" + 0.003*\"press\" + 0.003*\"ag\" + 0.003*\"display\" + 0.003*\"innov\" + 0.003*\"opportun\" + 0.002*\"http\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1, num_words=50):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=T_Text[\"Text\"]):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[bow_corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Percentage Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=T_Text[\"Text\"])\n",
    "\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic[\"Text\"] = df_dominant_topic[\"Text\"].str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>robot, research, system, digit, develop, desig...</td>\n",
       "      <td>[busi, function, art, textil, time, georgian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>robot, research, system, digit, develop, desig...</td>\n",
       "      <td>[milli, amp, root, headscarf, child, book, dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9049</td>\n",
       "      <td>robot, research, system, digit, develop, desig...</td>\n",
       "      <td>[pamex, epson, market, leader, textil, print, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>one, said, smart, look, say, go, show, want, p...</td>\n",
       "      <td>[pamplin, medium, group, k, cloth, boutiqu, gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3653</td>\n",
       "      <td>one, said, smart, look, say, go, show, want, p...</td>\n",
       "      <td>[sell, shirt, own, tune, tokyo, franki, quiroz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>global, analysi, research, industri, region, g...</td>\n",
       "      <td>[global, optic, brighten, market, reach, u, bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9135</td>\n",
       "      <td>one, said, smart, look, say, go, show, want, p...</td>\n",
       "      <td>[christ, bodyguard, men, protect, philippin, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7434</td>\n",
       "      <td>one, said, smart, look, say, go, show, want, p...</td>\n",
       "      <td>[th, chitra, santh, varieti, buyer, found, dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6862</td>\n",
       "      <td>robot, research, system, digit, develop, desig...</td>\n",
       "      <td>[heimtextil, launch, innov, new, trend, interi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3524</td>\n",
       "      <td>devic, wearabl, medic, smart, health, monitor,...</td>\n",
       "      <td>[fit, tracker, use, explod, u, especi, among, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0           0             4.0              0.9913   \n",
       "1           1             4.0              0.6575   \n",
       "2           2             4.0              0.9049   \n",
       "3           3             1.0              0.8880   \n",
       "4           4             1.0              0.3653   \n",
       "5           5             6.0              0.8337   \n",
       "6           6             1.0              0.9135   \n",
       "7           7             1.0              0.7434   \n",
       "8           8             4.0              0.6862   \n",
       "9           9             5.0              0.3524   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  robot, research, system, digit, develop, desig...   \n",
       "1  robot, research, system, digit, develop, desig...   \n",
       "2  robot, research, system, digit, develop, desig...   \n",
       "3  one, said, smart, look, say, go, show, want, p...   \n",
       "4  one, said, smart, look, say, go, show, want, p...   \n",
       "5  global, analysi, research, industri, region, g...   \n",
       "6  one, said, smart, look, say, go, show, want, p...   \n",
       "7  one, said, smart, look, say, go, show, want, p...   \n",
       "8  robot, research, system, digit, develop, desig...   \n",
       "9  devic, wearabl, medic, smart, health, monitor,...   \n",
       "\n",
       "                                                Text  \n",
       "0  [busi, function, art, textil, time, georgian, ...  \n",
       "1  [milli, amp, root, headscarf, child, book, dea...  \n",
       "2  [pamex, epson, market, leader, textil, print, ...  \n",
       "3  [pamplin, medium, group, k, cloth, boutiqu, gr...  \n",
       "4  [sell, shirt, own, tune, tokyo, franki, quiroz...  \n",
       "5  [global, optic, brighten, market, reach, u, bn...  \n",
       "6  [christ, bodyguard, men, protect, philippin, r...  \n",
       "7  [th, chitra, santh, varieti, buyer, found, dea...  \n",
       "8  [heimtextil, launch, innov, new, trend, interi...  \n",
       "9  [fit, tracker, use, explod, u, especi, among, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"LDA Result_Google.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
